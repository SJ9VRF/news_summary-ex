{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import json\n",
    "from math import log\n",
    "import sys\n",
    "import nltk, re, pprint\n",
    "from nltk import bigrams\n",
    "import urllib\n",
    "import copy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "from math import log\n",
    "import itertools\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "stopword_list = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mohsen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_news = 'Downloads/kerman/LDC2003T05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news(add):\n",
    "    \n",
    "    with open(add, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    news=[]    \n",
    "    n = len(data)\n",
    "    #n = 1500\n",
    "    for i in range(n):\n",
    "        if data[i].find('<TEXT>')!= -1 :\n",
    "            a_news = []\n",
    "            while(i+1<n and data[i+1].find('<\\TEXT>')==-1):\n",
    "                while(i+1<n and data[i+1].find('<\\P>')==-1):\n",
    "                    if data[i+1].find('<P>')!=-1:\n",
    "                        #the_str = \n",
    "                        a_news.append(data[i+2])\n",
    "                    i = i+1\n",
    "                    if i>=n:\n",
    "                        break\n",
    "            if  a_news != []:\n",
    "                news.append(a_news)\n",
    "            \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_news = read_news(add_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def expand_match(contraction):\n",
    "    match = contraction.group(0)\n",
    "    first_char = match[0]\n",
    "    expanded_contraction = contraction_mapping.get(match) \\\n",
    "    if contraction_mapping.get(match) \\\n",
    "    else contraction_mapping.get(match.lower())\n",
    "    expanded_contraction = first_char + expanded_contraction[1:]\n",
    "    \n",
    "    return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(mytext):\n",
    "    mytext = mytext.lower()\n",
    "    mytext = remove_special_characters(mytext)\n",
    "    mytext = remove_stopwords(mytext)\n",
    "    return mytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sent(the_txt):\n",
    "\n",
    "    the_txt = pre_process(the_txt)\n",
    "    desired_google_queries = the_txt.split()\n",
    "\n",
    "    the_score = 0\n",
    "    for query in desired_google_queries:\n",
    "\n",
    "        url = 'http://google.com/search?q=' + query\n",
    "\n",
    "        req = urllib.request.Request(url, headers={'User-Agent' : \"Magic Browser\"})\n",
    "        response = urllib.request.urlopen( req )\n",
    "        html = response.read()\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        resultStats = soup.find(id=\"resultStats\").string\n",
    "\n",
    "        the_score = the_score + (int(((resultStats.replace('About ','').replace('results','')).replace(',',''))))\n",
    "        n = len(desired_google_queries) \n",
    "        if n==0:\n",
    "            finalScore =  0\n",
    "        else :\n",
    "            finalScore =  the_score/n\n",
    "    return (finalScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def summ_new_item(item_score, news, num_sent_sum):\n",
    "    n = len(item_score)\n",
    "    indexs = np.argsort(item_score)[n-num_sent_sum, n]\n",
    "    my_sum = np.array(news[indexs])\n",
    "    \n",
    "    return my_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sum = []\n",
    "num_sent_sum=2\n",
    "\n",
    "for jtem in data_news:\n",
    "    item_score = []\n",
    "    for item in jtem:\n",
    "        item_score.append(score_sent(item))\n",
    "    all_sum.append(summ_new_item(item_score, news, num_sent_sum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
